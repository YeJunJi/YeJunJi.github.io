# Deep Reinforcement Learning with Double Q-learning

### Paper

[Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)

### Abstract

**문제**

1. 기존에 Q-learning은 action-value를 overestimate함, 또한 아래 문제들에 대한 해답이 없었음
    1. overestimate가 흔한지 아닌지
    2. overestimate가 performance에 악영향을 끼치는지 아닌지
    3. overestimate를 general하게 예방할 수 있는지 없는지

제안한 알고리즘: double DQN

1. 해결한 것
    1. large-scale function approximation에서 generalize
    2. reduce observed overestimations
    3. much better performance on several games
    

### Introduction

Q-value overestimation에 대한 이전 생각들

overestimation이 가져올 수 있는 문제들

1. insufficiently flexible function approximation
2. noise

**Cause of overestimation**

- action-values are incorrect
    - 매 학습마다 매우 빈번하게 발생(Abstract-a에 대한 해답)

**Double DQN을 통해 아래내용들을 실험적으로 증명**

1. more accurate value estimates(Abstract-b에 대한 해답)
2. reduce action-value overestimation(Abstract-c에 대한 해답)

### Double DQN

Nature DQN → same values both to select and to evaluate an action

문제점: select overestimated value

Double DQN → decouple the selection from the evaluation

- Double DQN’s TD-target

![Untitled](/assets/images/2022-01-21-DDQN/Untitled.png)

1. $\theta_t$ : selection of the action with online weight
    
    → esimating the value of greedy policy
    
2. $\theta^\prime_t$ : fairly evaluate the value of this policy
    
    → updated symmetrically by switching the roles of $\theta$ and $\theta^\prime$
    

### Overoptimism due to estimation errors

estimation errors(environment error, fuction approximation, non-stationarity)가 upward bias를 유도할 수 있음을 보다 일반적으로 설명

→ 초기에 true value를 평가하는 것에 있어서 불확실하기 때문에 학습할 때 오차가 발생

**Theorem1**

→ value estimation이 평균적으로 정확하더라도 어떤 source의 estimation errors가 추정치를 올려버리고 true optimal values에서 벗어나게 만드는 것을 증명

가정

1. $Q_*(s, a) = V_*(s)$
2. $\sum_a(Q_t(s, a)-V_*(s)) = 0$
3. ${1 \over m} \sum_a(Q_t(s, a)-V_*(s))^2 = C$

→ $max_aQ_t(s, a) \ge V_*(s)+\sqrt {C\over m-1}$

### Experiments

![Untitled](/Users/jiyejun/Desktop/github/YeJunJi.github.io/assets/images/2022-01-21-DDQN/Untitled 1.png)

Ground truth: obtained by running the best learned policies for several episodes 

- DDQN이 더 true value와 가깝게  측정하는 것을 볼 수 있음
- DDQN이 true value를 더 높게 측정 → better policy
- log-scale을 통해 DQN은 unstable한 것을 볼 수 있음

![Untitled](/Users/jiyejun/Desktop/github/YeJunJi.github.io/assets/images/2022-01-21-DDQN/Untitled 2.png)

- DQN이 value estimation을 계속 높게 잡아도 score는 떨어지는 것을 볼 수 있음
- overestimation → low performance

![Untitled](/Users/jiyejun/Desktop/github/YeJunJi.github.io/assets/images/2022-01-21-DDQN/Untitled 3.png)

- DQN, DDQN, tuned DDQN의 성능을 비교한 지표

![Untitled](/Users/jiyejun/Desktop/github/YeJunJi.github.io/assets/images/2022-01-21-DDQN/Untitled 4.png)